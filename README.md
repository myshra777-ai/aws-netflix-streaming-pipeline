

```markdown
# ğŸ¬ Netflix Streaming Data Pipeline on AWS

An **end-to-end, production-style data engineering project** that ingests Netflix-style streaming events into an **S3 data lake**, transforms them with **AWS Glue**, stores optimized **Parquet files**, and exposes the data for analytics via **Amazon Athena**.  

![AWS](https://img.shields.io/badge/AWS-Glue%20%7C%20Athena%20%7C%20S3-orange?logo=amazonaws)  
![Status](https://img.shields.io/badge/Status-Active%20Development-brightgreen)  
![License](https://img.shields.io/badge/License-MIT-blue)

---

## ğŸš© Problem Statement
Design a **production-grade ETL pipeline** for Netflix viewing events:

- ğŸ“¥ Ingest raw streaming events into an **S3 data lake (bronze layer)**  
- ğŸ”„ Transform and optimize data using **AWS Glue (silver layer)**  
- ğŸ“Š Store data in **Parquet format** for fast analytics  
- ğŸ” Query the data using **Amazon Athena (gold layer)**  
- â° Add **scheduling & monitoring** to make the pipeline production-ready  

---

## ğŸ—ï¸ Architecture Overview

**Region:** `ap-south-1 (Mumbai)`  
**Bucket:** `myshr-netflix-datalake-ap-south-1`

```mermaid
flowchart TD
    A[Raw Events (CSV/JSON)] -->|Glue Crawler| B[Raw Athena Table]
    B -->|Glue ETL Job| C[Processed Parquet in S3]
    C -->|Glue Crawler| D[Processed Athena Tables]
    D --> E[Analytics Queries in Athena]
    E --> F[CloudWatch Alarms + SNS Notifications]
```

Layers:
- **Bronze (Raw Layer)** â†’ S3 + Glue crawler + Athena raw table  
- **Silver (Processed Layer)** â†’ Parquet + Glue ETL job + Athena processed tables  
- **Gold (Analytics Layer)** â†’ Athena queries for insights  
- **Ops Layer** â†’ Glue triggers + CloudWatch alarms  

---

## ğŸ“ Data Model

**Raw Events Schema:**
- `user_id` ğŸ”‘  
- `title_id` ğŸï¸  
- `event_type` (e.g., `PLAY_START`)  
- `event_ts` (epoch timestamp)  
- `device_type` (mobile, web, TV)  
- `country` (ISO code)  

**Processed Layer:** Same schema, stored in **Parquet + Snappy compression** for efficiency.

---

## âš™ï¸ AWS Glue ETL Job (PySpark)

ğŸ“‚ Script: `etl/netflix_raw_to_processed.py`

**Steps:**
1. Read raw events from Glue Data Catalog (`raw_netflix_events_rootevents`)  
2. Apply basic data quality checks (`ColumnCount > 0`)  
3. Write to S3 in **Parquet (Snappy)** under `processed/`  
4. Update Athena tables via Glue crawler  

---

## ğŸ“Š Athena Analytics Queries

Stored in `sql/`:

- **Total Events**
  ```sql
  SELECT COUNT(*) AS total_events
  FROM raw_netflix_events_rootevents;
  ```

- **Top Movies**
  ```sql
  SELECT title_id, COUNT(*) AS views
  FROM raw_netflix_events_rootevents
  WHERE event_type = 'PLAY_START'
  GROUP BY title_id
  ORDER BY views DESC
  LIMIT 10;
  ```

- **Country Distribution**
  ```sql
  SELECT country, COUNT(*) AS events
  FROM raw_netflix_events_rootevents
  GROUP BY country
  ORDER BY events DESC;
  ```

âœ… Example Results:
- 3,010 total events  
- All `PLAY_START` events â†’ `title_id = 100`  
- All events from `country = 'IN'`  

---

## â±ï¸ Scheduling & Monitoring

- **Trigger:** `netflix-daily-etl-trigger` â†’ runs Glue job daily at **02:00 AM UTC (07:30 AM IST)**  
- **CloudWatch Alarm:** `netflix-glue-job-high-resource-usage` â†’ monitors Glue job resource usage (>50% threshold)  
- **SNS Notifications:** Email alerts on job failures or anomalies  

---

## ğŸ“‚ Repository Structure

```text
aws-netflix-streaming-pipeline/
â”œâ”€ etl/
â”‚  â””â”€ netflix_raw_to_processed.py      # Glue PySpark ETL job
â”œâ”€ sql/
â”‚  â”œâ”€ total_events.sql                 # Total events
â”‚  â”œâ”€ top_movies.sql                   # Top titles by views
â”‚  â””â”€ country_distribution.sql         # Events per country
â”œâ”€ config/
â”‚  â””â”€ netflix_config.json              # Future pipeline configuration
â””â”€ docs/
   â””â”€ README.md                        # Project documentation
```

---

## ğŸš€ How to Run

1. **Raw Layer Setup** â†’ Upload raw files â†’ Run Glue crawler  
2. **ETL Job** â†’ Execute `netflix_raw_to_processed.py` â†’ Write Parquet to S3  
3. **Processed Crawler** â†’ Update Athena tables  
4. **Analytics** â†’ Run queries in Athena  
5. **Automation** â†’ Enable Glue trigger for daily runs  

---

## ğŸ”® Future Enhancements
- Advanced transformations (aggregations, serving schema)  
- Partitioning by `event_date` / `country` for Athena cost optimization  
- Richer Glue Data Quality rules (null checks, enums, ranges)  
- BI dashboards in **QuickSight** or **Grafana**  

---

## ğŸ’¼ Portfolio Use

> â€œBuilt an end-to-end Netflix streaming data pipeline on AWS using S3, Glue, and Athena. Implemented a multi-layer data lake (raw, processed, analytics), automated daily ETL with Glue triggers, stored data in Parquet with Snappy compression, and set up CloudWatch monitoring for Glue job resource usage.â€

---

```

---

This version adds:
- ğŸ¨ **Visual polish**: emojis, badges, Mermaid diagram.  
- ğŸ“Š **Clear sections**: icons for readability.  
- âœ… **Recruiter-ready highlights**: portfolio callâ€‘out at the end.  

